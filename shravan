import json
import logging
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from .document_service import DocumentService
from .embedding_service import EmbeddingService
import re

from .vector_store import VectorStore

# Configure logging
logger = logging.getLogger(__name__)

class Message(BaseModel):
    role: str  # "user" or "assistant"
    content: str


class ChatService:
    """
    Service for managing chat interactions and generating responses.
    Integrates with the vector store to retrieve relevant documents for answering questions.
    """
    
    def __init__(self, vector_store: VectorStore, embedding_service: EmbeddingService):
        """
        Initialize the chat service.
        
        Args:
            vector_store: Vector store for document retrieval
            embedding_service: Service for generating embeddings and text completions
        """
        self.vector_store = vector_store
        self.embedding_service = embedding_service
        self.conversation_history = []
        logger.info("ChatService initialized")
    
    def add_message(self, message: Dict[str, str]) -> None:
        """
        Add a message to the conversation history.
        
        Args:
            message: Message object with role and content
        """
        self.conversation_history.append(message)
        logger.info(f"Added message to history - Role: {message['role']}, Content length: {len(message['content'])}")
        
        # Limit conversation history to last 10 messages
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
            logger.info("Trimmed conversation history to last 10 messages")
    
#     def generate_response(self, query: str, category_filter: Optional[str] = None) -> Dict[str, Any]:
#         """
#         Generate a response to a user query.
        
#         Args:
#             query: User's question
#             category_filter: Optional category to filter documents by
            
#         Returns:
#             Dict containing response text and source information
#         """
#         logger.info(f"Generating response for query: '{query}'")
#         if category_filter:
#             logger.info(f"Using category filter: {category_filter}")
        
#         # Add user message to history
#         self.add_message({"role": "user", "content": query})
        
#         # Get relevant document chunks from vector store
#         logger.info("Retrieving relevant documents from vector store")
#         relevant_docs = self.vector_store.similarity_search(
#             query=query,
#             embedding_fn=self.embedding_service.get_query_embedding,
#             top_k=5,
#             category=category_filter
#         )
        
#         # Store relevant docs for later use in fallback parsing
#         self.last_relevant_docs = relevant_docs
        
#         # for the doc not found case, we can handle it gracefully

#         if not relevant_docs:
#             logger.warning("No relevant documents found for query")
#             response = {
#                 "answer": "I don't have information on that topic in my HR policy database. Please check with your HR department directly.",
#                 "sources": []
#             }
#             self.add_message({"role": "assistant", "content": response["answer"]})
#             return response
        
       
#         logger.info(f"Found {len(relevant_docs)} relevant documents")
        
#         # Analyze query intent - check if the query is ambiguous or clear
#         query_analysis = self._analyze_query_intent(query, relevant_docs)
        
#         # Format conversation history for context
#         conversation_context = "\n".join([
#             f"{msg['role'].upper()}: {msg['content']}" 
#             for msg in self.conversation_history[-6:-1]  # Exclude current query
#         ])
        
#         # Format relevant documents for context
#         docs_context = []
#         for i, doc in enumerate(relevant_docs):
#             # Log document details
#             doc_title = doc['metadata'].get('title', 'Untitled')
#             doc_score = doc.get('score', 0.0)
#             logger.info(f"Document {i+1}: {doc_title} (Score: {doc_score:.4f})")
            
#             # Get document content (handle both 'content' and 'text' fields)
#             doc_content = doc.get('content', doc.get('text', ''))
#             chunk_info = ""
#             # Add chunk information if available
#             if 'chunk_index' in doc['metadata'] and 'total_chunks' in doc['metadata']:
#                 chunk_info = f" (Chunk {doc['metadata']['chunk_index']+1} of {doc['metadata']['total_chunks']})"
            
#             # Format document with metadata
#             source_info = f"DOCUMENT {i+1}: {doc_title}{chunk_info}"
#             if 'page' in doc['metadata']:
#                 source_info += f" (Page {doc['metadata']['page']})"
#             if 'section' in doc['metadata']:
#                 source_info += f", Section: {doc['metadata']['section']}"
                
#             docs_context.append(f"{source_info}\n{doc_content}")
        
#         docs_text = "\n\n".join(docs_context)
#         logger.info(f"Created context with {len(docs_text)} characters from {len(docs_context)} documents")
        
#         # Construct the prompt for Gemini
#         prompt = f"""You are an HR Policy Assistant that helps employees understand company policies.
# Use ONLY the information from the documents provided below to answer the user's question.
# If the answer isn't contained in the documents, respond with: "I don't have information on that topic in the HR policy database. Please check with your HR department directly."

# CONVERSATION HISTORY:
# {conversation_context}

# USER QUERY: {query}

# RELEVANT HR POLICY DOCUMENTS:
# {docs_text}

# INSTRUCTIONS:
# 1. Answer ONLY based on the provided documents.
# 2. Be precise and cite specific policy details.
# 3. Use clear formatting and structure.
# 4. ALWAYS cite your sources using the format [Document X, Page Y, "Section Z"] after each key point.
# 5. When multiple documents are relevant, cite each one.
# 6. If the query is ambiguous, provide clarification questions.

# {query_analysis}

# Return your response in the following JSON format:
# {{
#   "answer": "Your clear, concise answer with specific policy details and properly formatted citations [Document X, Page Y, 'Section Z']",
#   "sources": [
#     {{
#       "title": "Document title",
#       "category": "Document category",
#       "page": page_number,
#       "section": "Section title if available",
#       "text": "Relevant excerpt from the document",
#       "relevance_score": 0.95,
#       "relevance": "Brief explanation of why this source is relevant to the query"
#     }}
#   ],
#   "follow_up_questions": ["Question 1?", "Question 2?", "Question 3?"]
# }}

# IMPORTANT: Ensure your answer directly addresses the user's specific question with precise policy information and includes at least 2-3 follow-up questions related to the topic."""

#         logger.info("Constructed enhanced prompt for Gemini model")
        
#         # Generate response using embedding service
#         logger.info("Generating text with Gemini API")
#         response_text = self.embedding_service.generate_text(prompt)
#         logger.info(f"Received response of length: {len(response_text)}")
        
#         # Parse the response
#         try:
#             # Try to parse as JSON first
#             logger.info("Attempting to parse response as JSON")
#             parsed_response = json.loads(response_text)
#             if isinstance(parsed_response, dict) and "answer" in parsed_response:
#                 logger.info("Successfully parsed response as JSON")
#                 result = parsed_response
#             else:
#                 # If JSON doesn't have expected structure, try alternative parsing
#                 logger.warning("Response JSON missing 'answer' field, using fallback parsing")
#                 result = self._parse_response(response_text)
#         except json.JSONDecodeError:
#             # If not valid JSON, use alternative parsing
#             logger.warning("Failed to parse response as JSON, using fallback parsing")
#             result = self._parse_response(response_text)
        
#         # Add assistant message to history
#         self.add_message({"role": "assistant", "content": result["answer"]})
        
#         logger.info(f"Final response contains {len(result.get('sources', []))} sources")
#         return result
    
    def generate_response(self, query: str, category_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        Generate a response to a user query.

        Args:
            query: User's question
            category_filter: Optional category to filter documents by

        Returns:
            Dict containing response text and source information
        """
        logger.info(f"Generating response for query: '{query}'")
        if category_filter:
            logger.info(f"Applying category filter: {category_filter}")

        # Add user query to conversation history
        self.add_message({"role": "user", "content": query})

        # Retrieve top relevant documents
        logger.info("Searching for relevant documents using vector store...")
        relevant_docs = self.vector_store.similarity_search(
            query=query,
            embedding_fn=self.embedding_service.get_query_embedding,
            top_k=5,
            category=category_filter
        )

        # Cache for fallback usage
        self.last_relevant_docs = relevant_docs

        # ==== CASE 1: No documents found â†’ Use Gemini HR Expert ====
        if not relevant_docs:
            logger.warning("No relevant documents found. Using Gemini fallback.")

            fallback_prompt = f"""
            You are a professional HR Assistant designed to help employees with their HR-related queries.
            The user asked:
            "{query}"

            There were no matches found in the internal HR document database.
            Please generate a professional, helpful, and policy-aligned response based on your general HR expertise.

            Instructions:
            - Be clear and concise.
            - Maintain a formal and supportive tone.
            - Suggest next steps or departments to contact if appropriate.
            - If the query is too specific, generalize your answer while acknowledging limitations.

            Return your response in this JSON format:
            {{
                "answer": "Professional and clear response.",
                "sources": [],
                "follow_up_questions": ["Related question 1", "Related question 2", "Related question 3"]
            }}
            """

            response_text = self.embedding_service.generate_text(fallback_prompt)
            logger.info("Fallback response received. Attempting to parse...")

            try:
                parsed_response = json.loads(response_text)
                if "answer" in parsed_response:
                    result = parsed_response
                else:
                    logger.warning("Parsed fallback JSON lacks 'answer'. Using fallback parser.")
                    result = self._parse_response(response_text)
            except json.JSONDecodeError:
                logger.warning("Fallback response is not valid JSON. Using fallback parser.")
                result = self._parse_response(response_text)

            self.add_message({"role": "assistant", "content": result.get("answer", "Unable to provide a response.")})
            return result

        # ==== CASE 2: Relevant documents found â†’ Use document-grounded prompt ====
        logger.info(f"{len(relevant_docs)} relevant documents retrieved")

        query_analysis = self._analyze_query_intent(query, relevant_docs)

        # Format recent conversation history for context (excluding current query)
        conversation_context = "\n".join([
            f"{msg['role'].upper()}: {msg['content']}"
            for msg in self.conversation_history[-6:-1]
        ])

        # Format document context
        docs_context = []
        for i, doc in enumerate(relevant_docs):
            title = doc['metadata'].get('title', 'Untitled')
            score = doc.get('score', 0.0)
            chunk_info = ""
            if 'chunk_index' in doc['metadata'] and 'total_chunks' in doc['metadata']:
                chunk_info = f" (Chunk {doc['metadata']['chunk_index']+1} of {doc['metadata']['total_chunks']})"

            page_info = f" (Page {doc['metadata']['page']})" if 'page' in doc['metadata'] else ""
            section_info = f", Section: {doc['metadata']['section']}" if 'section' in doc['metadata'] else ""

            source_header = f"DOCUMENT {i+1}: {title}{chunk_info}{page_info}{section_info}"
            content = doc.get('content', doc.get('text', ''))

            docs_context.append(f"{source_header}\n{content}")
            logger.info(f"Doc {i+1}: '{title}' | Score: {score:.4f}")

        docs_text = "\n\n".join(docs_context)

        # Compose document-based Gemini prompt
        prompt = f"""
        You are an HR Policy Assistant that helps employees understand company policies.
        Use ONLY the information from the documents provided below to answer the user's question.
        If the answer isn't contained in the documents, respond with: 
        "I don't have information on that topic in the HR policy database. Please check with your HR department directly."

        CONVERSATION HISTORY:
        {conversation_context}

        USER QUERY: {query}

        RELEVANT HR POLICY DOCUMENTS:
        {docs_text}

        INSTRUCTIONS:
        1. Answer ONLY based on the provided documents.
        2. Be precise and cite specific policy details.
        3. Use clear formatting and structure.
        4. ALWAYS cite your sources using the format [Document X, Page Y, "Section Z"] after each key point.
        5. When multiple documents are relevant, cite each one.
        6. If the query is ambiguous, provide clarification questions.

        {query_analysis}

        Return your response in the following JSON format:
        {{
        "answer": "Your clear, concise answer with specific policy details and properly formatted citations [Document X, Page Y, 'Section Z']",
        "sources": [
            {{
            "title": "Document title",
            "category": "Document category",
            "page": page_number,
            "section": "Section title if available",
            "text": "Relevant excerpt from the document",
            "relevance_score": 0.95,
            "relevance": "Brief explanation of why this source is relevant to the query"
            }}
        ],
        "follow_up_questions": ["Question 1?", "Question 2?", "Question 3?"]
        }}

        IMPORTANT: Ensure your answer directly addresses the user's specific question with precise policy information and includes at least 2-3 follow-up questions related to the topic.
        """

        logger.info("Calling Gemini with composed document-based prompt...")
        response_text = self.embedding_service.generate_text(prompt)
        logger.info(f"Response received ({len(response_text)} characters)")

        try:
            parsed_response = json.loads(response_text)
            if "answer" in parsed_response:
                result = parsed_response
                logger.info("Successfully parsed Gemini response as JSON.")
            else:
                logger.warning("Gemini response JSON missing 'answer'. Using fallback parser.")
                result = self._parse_response(response_text)
        except json.JSONDecodeError:
            logger.warning("Gemini response not in valid JSON format. Using fallback parser.")
            result = self._parse_response(response_text)

        # Check if the answer is the fallback phrase, if so, use Gemini fallback
        fallback_phrase = "I don't have information on that topic in the HR policy database. Please check with your HR department directly."
        if result.get("answer", "").strip().startswith(fallback_phrase):
            logger.info("Detected fallback phrase in Gemini answer. Using Gemini HR Expert fallback.")
            fallback_prompt = f"""
            You are a professional HR Assistant designed to help employees with their HR-related queries.
            The user asked:
            "{query}"

            The answer was not found in the internal HR document database.
            Please generate a professional, helpful, and policy-aligned response based on your general HR expertise.

            Instructions:
            - Be clear and concise.
            - Maintain a formal and supportive tone.
            - Suggest next steps or departments to contact if appropriate.
            - If the query is too specific, generalize your answer while acknowledging limitations.

            Return your response in this JSON format:
            {{
                "answer": "Professional and clear response.",
                "sources": [],
                "follow_up_questions": ["Related question 1", "Related question 2", "Related question 3"]
            }}
            """
            response_text = self.embedding_service.generate_text(fallback_prompt)
            try:
                parsed_response = json.loads(response_text)
                if "answer" in parsed_response:
                    result = parsed_response
                else:
                    result = self._parse_response(response_text)
            except json.JSONDecodeError:
                result = self._parse_response(response_text)

        self.add_message({"role": "assistant", "content": result.get("answer", "Response could not be generated.")})
        logger.info(f"Final response compiled with {len(result.get('sources', []))} source(s)")
        return result



# below code is original code 
    def _analyze_query_intent(self, query: str, relevant_docs: List[Dict]) -> str:
        """
        Analyze the user's query intent to determine if it's ambiguous or clear.
        
        Args:
            query: The user's query
            relevant_docs: The retrieved relevant documents
            
        Returns:
            String with additional instructions based on query analysis
        """
        # Simple heuristics for ambiguity detection
        query_lower = query.lower()
        ambiguous_terms = ["policy", "rule", "guideline", "procedure", "what is", "tell me about"]
        question_words = ["who", "what", "when", "where", "why", "how"]
        
        # Check if the query contains ambiguous terms
        contains_ambiguous_terms = any(term in query_lower for term in ambiguous_terms)
        
        # Check if the query is a simple question
        is_simple_question = any(query_lower.startswith(word) for word in question_words)
        
        # Check if there's a wide variation in document relevance scores
        if len(relevant_docs) > 1 and "score" in relevant_docs[0]:
            top_score = relevant_docs[0].get("score", 0)
            last_score = relevant_docs[-1].get("score", 0)
            score_variation = top_score - last_score
            high_variation = score_variation > 0.3  # More than 0.3 difference is considered high
        else:
            high_variation = False
        
        # Additional instructions based on analysis
        if contains_ambiguous_terms and not is_simple_question:
            return """HANDLING AMBIGUOUS QUERY:
This query is potentially ambiguous. In your response:
1. Acknowledge the ambiguity
2. Provide clarification questions to narrow down the topic
3. Give a general overview based on the most relevant policies
4. Suggest more specific follow-up questions"""
        elif high_variation:
            return """HANDLING VARIED RELEVANCE:
The documents have varying relevance to this query. In your response:
1. Focus primarily on the most relevant documents
2. Briefly mention related information from lower-relevance documents if helpful
3. Suggest specific follow-up questions to explore other aspects"""
        else:
            return """HANDLING SPECIFIC QUERY:
This query appears specific. In your response:
1. Provide a direct, thorough answer addressing the specific question
2. Include all policy details relevant to the query
3. Suggest related follow-up questions that might be of interest"""
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse the response text to extract answer and sources.
        Used as a fallback when JSON parsing fails.
        
        Args:
            response_text: Response text from model
            
        Returns:
            Dict with answer and sources
        """
        logger.info("Parsing response text using fallback method")
        
        # Check if response is wrapped in a code block
        code_block_pattern = r'```(?:json)?\s*([\s\S]*?)```'
        code_block_match = re.search(code_block_pattern, response_text)
        
        if code_block_match:
            # Found a code block - try to parse it as JSON
            code_content = code_block_match.group(1).strip()
            logger.info("Found code block, attempting to parse as JSON")
            try:
                parsed_json = json.loads(code_content)
                if isinstance(parsed_json, dict) and "answer" in parsed_json:
                    logger.info("Successfully parsed JSON from code block")
                    return parsed_json
                logger.warning("Code block JSON missing 'answer' field")
            except json.JSONDecodeError:
                logger.warning("Failed to parse code block as JSON")
        
        # Default structure
        result = {
            "answer": response_text,
            "sources": []
        }
        
        # Try to extract enhanced format citations using regex
        # Format: [Document X, Page Y, "Section Z"]
        enhanced_pattern = r'\[Document (\d+)(?:, Page (\d+))?(?:, ["\']([^"\']+)["\'])?\]'
        enhanced_matches = re.findall(enhanced_pattern, response_text)
        
        # Fall back to basic pattern if no enhanced matches
        if not enhanced_matches:
            basic_pattern = r'\[Document (\d+)\]'
            basic_matches = re.findall(basic_pattern, response_text)
            logger.info(f"Found {len(basic_matches)} basic source citations in text")
            
            # Convert to enhanced format
            enhanced_matches = [(doc_idx, '', '') for doc_idx in basic_matches]
        else:
            logger.info(f"Found {len(enhanced_matches)} enhanced source citations in text")
        
        # Get unique document citations (some may be referenced multiple times)
        unique_citations = {}
        for doc_idx, page, section in enhanced_matches:
            if doc_idx not in unique_citations:
                unique_citations[doc_idx] = {
                    'doc_idx': int(doc_idx),
                    'pages': set(),
                    'sections': set()
                }
            
            if page:
                unique_citations[doc_idx]['pages'].add(int(page))
            
            if section:
                unique_citations[doc_idx]['sections'].add(section)
        
        # If we have relevant docs and document indexes from citations
        if unique_citations and hasattr(self, 'last_relevant_docs'):
            logger.info(f"Processing {len(unique_citations)} unique document citations")
            
            for doc_idx, citation_info in unique_citations.items():
                idx = int(doc_idx)
                if 0 < idx <= len(self.last_relevant_docs):
                    doc = self.last_relevant_docs[idx-1]
                    
                    # Create a source entry with available metadata
                    source = {
                        "title": doc["metadata"].get("title", doc["metadata"].get("document_name", "Untitled")),
                        "category": doc["metadata"].get("category", "General"),
                        "relevance": "Directly cited in the answer"  # Default relevance
                    }
                    
                    # Add page and section information
                    # First from citation if available
                    if citation_info['pages']:
                        source["pages"] = list(citation_info['pages'])
                        source["page"] = min(citation_info['pages'])  # For backward compatibility
                    # Then from document metadata if not in citation
                    elif "page" in doc["metadata"]:
                        source["page"] = doc["metadata"]["page"]
                    if "pages" in doc["metadata"] and not citation_info['pages']:
                        source["pages"] = doc["metadata"]["pages"]
                    
                    # Handle sections similarly
                    if citation_info['sections']:
                        source["sections"] = list(citation_info['sections'])
                        source["section"] = next(iter(citation_info['sections']))  # First section for compatibility
                    elif "section" in doc["metadata"]:
                        source["section"] = doc["metadata"]["section"]
                    if "sections" in doc["metadata"] and not citation_info['sections']:
                        source["sections"] = doc["metadata"]["sections"]
                    
                    # Add chunk information if available
                    if "chunk_index" in doc["metadata"] and "total_chunks" in doc["metadata"]:
                        source["chunk"] = f"{doc['metadata']['chunk_index']+1}/{doc['metadata']['total_chunks']}"
                    
                    # Add the source document content (truncated)
                    content = doc.get('content', doc.get('text', ''))
                    source["text"] = content[:200] + "..." if len(content) > 200 else content
                    
                    result["sources"].append(source)
                    logger.info(f"Added source: {source['title']}")
        
        # If no sources found but we have relevant docs, use them
        elif hasattr(self, 'last_relevant_docs'):
            logger.info("No source citations found, using top relevant documents")
            for i, doc in enumerate(self.last_relevant_docs[:3]):
                # Create source entry in the same format as above
                source = {
                    "title": doc["metadata"].get("title", doc["metadata"].get("document_name", "Untitled")),
                    "category": doc["metadata"].get("category", "General"),
                    "relevance": "Retrieved as relevant to the query"
                }
                
                # Add metadata as above
                if "page" in doc["metadata"]:
                    source["page"] = doc["metadata"]["page"]
                if "pages" in doc["metadata"]:
                    source["pages"] = doc["metadata"]["pages"]
                if "section" in doc["metadata"]:
                    source["section"] = doc["metadata"]["section"]
                if "sections" in doc["metadata"]:
                    source["sections"] = doc["metadata"]["sections"]
                
                # Add chunk information
                if "chunk_index" in doc["metadata"] and "total_chunks" in doc["metadata"]:
                    source["chunk"] = f"{doc['metadata']['chunk_index']+1}/{doc['metadata']['total_chunks']}"
                
                # Add the source document content (truncated)
                content = doc.get('content', doc.get('text', ''))
                source["text"] = content[:200] + "..." if len(content) > 200 else content
                
                result["sources"].append(source)
                logger.info(f"Added source: {source['title']}")
        
        return result
    
    def clear_history(self) -> None:
        """
        Clear the conversation history.
        """
        logger.info("Clearing conversation history")
        self.conversation_history = []
